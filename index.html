<!DOCTYPE html>
<html>

<head>
	<title>Jai for highlight.js</title>
	<!--
		<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css">
		<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
		<script type="text/javascript" src="jai.min.js"></script>
	-->
	<link rel="stylesheet" type="text/css" href="jaiEverything.css" />
	<style type="text/css">
		#notIt {
			display: none;
		}

		body,
		pre {
			margin: 0;
			padding: 0;
			display: grid;
			height: 100vh;
			font-weight: bold;
			font-size: 11px;
		}
	</style>
	<script type="text/javascript" src="localHilightDebug.js"></script>
	<script type="text/javascript" src="jai.js"></script>
</head>

<body>
	<pre id="notIt"><code class="language-jai">
for :positive_vibes_only v, n: holder print("[%] %\n", n, v);

atomic_or :: (dest: *$T, value: T)
#modify { return IsAtomicScalar(T); }
{
	#if CPU == .X64 {
		#asm { lock_or?T [dest], value; }
	} else {
		old := dest.*;
		while true {
			new := old | value;
			success := compare_and_swap(dest, old, new);
			if success  break;
			old = dest.*;
		}
	}
}

Fr\ ed :: enum_flags {
	A :: 1;
	B;
};

makeBucket\  Allocator :: inline #no_abc () -&gt; Alloc\ ator {
	ba := New(BucketAllocator);
	init(ba);
	return .{ pr\ oc = bucket_allocator_proc, data = xx ba };
} @note @"note with spaces"

X\ Y :: 5;

ma\ in();

main :: (a: u8) {
	newContext := New(context);
	other(42, .VALUE);
}
other :: /*x*/(a: b) -> int #dump {
	x: myInt;
	newContext := context;
}

Fruit_\ Transaction :: struct {
	#as using pos: Vector3;
	using #as vel: Vector3;
	fruit:    Fruits_We_Sell;
	quantity: u64;
}
		</code></pre>
	<pre id="it"><code class="language-jai">42;
cast//
	,//
	FORCE//
	(//
	*Table//
	(string, bool)//
	)//
	*f;

cast//
	(//
	*Table//
	(string, bool)//
	,//
	*f//
	,//
	FORCE//
	)//
	;

1//
	.(//
	u8//
	,//
	no_check//
	)//
	;

/*
// Instead of the intrinsics you'd find in other compilers, we provide functionality for robust inline assembly that
// tightly integrates with the rest of the high level language. The compiler will produce the same output for your
// asm code bit-for-bit every time.
//
// Note that we currently only target x86-64, so it is implied that you are working in that instruction set. This is
// not necessarily true and when the time comes other instruction sets will be added.
*/

basics :: () {
    // Let's start with some basic usage...

    // We tell the compiler that we are going to be writing assembly with the #asm directive as follows:
    #asm {
        // All statements in this block are in the familiar 'mnemonic operand, operand, operand, ...' form.

        // Here is our first instruction, a 64-bit immediate (imm) to general purpose register (gpr) move. The 'apple'
        // is our named gpr. We do automatic register mapping, so you can use familiar high level names just like you do
        // in regular programming.
        mov apple:, 10;

        // We can actually write that a bit more verbose, since 'apple:' is actually an inferred declaration in the same
        // way that 'apple := 10' is in the high level langauge. The instruction selector finds a match, and in this
        // case we got a gpr.

        // There is also an implicit size being used here. In traditional assembly you specifiy operand sizes through
        // the names of the registers (ax/eax/rax). Since we don't use those names, the size is appened onto the
        // mnemonic.
        //
        // Since we are in 64-bit mode, a 64-bit operand size is the default implicit size being used with scalar ops.
        // When dealing with vector ops, either 128, 256, or 512 is used depending on your feature set (we'll talk about
        // that later).

        // Here is the version that declares the register up front and uses an explicit operand size:
        banana: gpr;
        mov.64  banana, 17;
    }

    // In order to make use of our hard work done in #asm, we have to interop with the high level language a bit.

    // Here is our high level variable. This variable has space on the stack, and is also moved in and out of gpr's by
    // the high level code generator. With respect to #asm blocks, any variable referred to will be moved into the right
    // gpr for you. In a release build you can expect your variables to always be in the right registers, though it is
    // a good idea to double check code gen if you care.
    count := 10;

    #asm {
        // Here we add 17 to our count. Remember that 'add' is implicitly using a 64-bit operand size, and count is
        // mapped and loaded into the right gpr.
        add count, 17;
    }

    {
        // If we print 'count', its address gets taken (because it is cast to Any, which is a data
        // structure that points at 'count'); which causes the compiler to enforce
        // that 'count' is stored in memory, which means you can't use it as a register.
        // (Yes this is a complicated issue to try to explain so early on, but, you're likely
        // to run into this, so let's get it out of the way.) Here we just assign to a new variable
        // to_print, which has *its* address taken, and then count is fine.

        to_print := count;
        print("count is %\n", to_print);
    }

    // Another thing to note is that #asm blocks aren't actually new scopes. The registers declared in them are visible
    // in whatever scope the #asm block itself is in. It is possible for registers declared in #asm blocks to spill to
    // the stack before being used again in a later block. This is up to the code generator. In practice this only
    // happens when there is contention with other high level code (i.e. you are doing floating point math which uses
    // vector registers, or there is a call and registers need to be saved).

    // Let's make use of apple and banana...
    #asm {
        sub banana, apple;
        add count, banana;
    }

    to_print := count;
    print("count with apple and banana is %\n", to_print);

    // Instructions are selected based on the mnemonic and operands provided. Instruction mnemonics are intentionally
    // identical to the official mnemonic provided by Intel and AMD. This is so you can refer to official manuals when
    // programming rather than have to indirectly go through something like the intrinsics guide. This also means we
    // don't provide a listing ourselves. If an instruction is not being recognized by us, either we haven't implemented
    // it yet or we missed it.

    // Each instruction mnemonic has a set of forms where different operand combinations can be used. The compiler will
    // print out the potential set of forms when an instruction isn't quite matching what you gave it.
    //
    // Here are some basic examples of what you might see from the compiler...
    //
    //     gpr, gpr/mem     - first operand gpr, second operand gpr or memory
    //     gpr, imm8        - first operand gpr, second operand 8-bit immediate
    //     gpr.a, gpr/mem   - first operand gpr pinned to a, second operand gpr or memory

    // One difference you will find with the official documentation is that there are not 'v' prefixes on our mnemonics.
    // In traditional assemblers, you use the 'v' prefix to indicate that you want VEX encoding for the instruction.
    // Here, we determine if we use VEX encoding based on the feature set. If VEX is supported in the feature set (AVX
    // and up), you will get the VEX encoded version of the instruction, otherwise you'll get the legacy version. More
    // on feature sets later.
}

immediates :: () {
    // As you know, immediate operands are encoded directly into the instruction stream. Different instructions use
    // different sized immediates. Let's look at some basic examples...
    #asm {
        // The byte size variant of mov has a form that takes an 'imm8'. Signed and unsigned values are allowed here
        // since from the perspective of the instruction, the value is opaque. It just needs to fit in 8 bits.
        mov.8 b1:,  255;
        mov.8 b2:, -127;

        // If you tried a value too large, the compiler would tell you that you are trying to use an 'imm16' and there
        // are no matches on the mov.8 instruction, as you'd expect.
        // mov.8 b3:, 256; // This line will error.

        // Here are the other mov's for the different sizes...
        mov.16 w1:,  65535;
        mov.16 w2:, -32768;
        mov.32 d1:,  4294967295;
        mov.32 d2:, -2147483648;
        mov.64 q3:,  18446744073709551615;
        mov.64 q4:, -9223372036854775808;

        // For the 64-bit 'mov.64', there is an additional form that takes a sign-extended imm32. This is a pretty common
        // form in the instruction set. Most instructions do sign extensions, the previous mov forms we've seen are an
        // exception. The implication of the sign-extension is that the biggest positive value is now the signed max,
        // not the unsigned max.
        mov.64 q1:,  2147483647;
        mov.64 q2:, -2147483648;

        // It is ambiguous if you want the 32-bit sign-extended version or the 64-bit version when your value is valid
        // in both cases. The instruction selector priortizes the 32-bit sign-extended version since the instruction is
        // smaller (4 byte vs 8 byte immediate). The 64-bit immediate version also doesn't support mov's to memory.

        // We can also do float immediates when the operand is opaque to the instruction. This limits it to instructions
        // that don't do sign-extensions. The 32-bit immediate form gets a single (float32) and the 64-bit immediate
        // form gets a double (float64).
        mov.32 f1:, 133.247;
        mov.64 f2:, 133.247;

        // If we tried to use a double in an imm32, the compiler will let us know 64-bits is required.
        // mov.32 f3:, 133.247247247247247247247247247247247; // This line will error.
    }
}

allocation_and_pinning :: () {
    // The register allocator has some smarts and takes things like lifetimes into account. This lets you use new names
    // to convey data flow the same way you would in high level code. As a result, register management is turned into a
    // working set size problem rather than an annoying book-keeping one. If you ever exceed the maximum number of alive
    // registers, you will get an error from the compiler. There is no automatic spilling or other magic going on here.
    #asm {
        mov a:, 32; // alloc a
        mov b:, 64; // alloc b, free b
        add a,  10;
        mov c:, 16; // alloc c
        add a,  c;  // free  a, free c
    }

    // So far we have only seen gpr's, but there are other register classes, each with a separate pool for allocation.
    // Here they all are:
    //
    //     gpr - general purpose register,  pool: 8 in 32-bit mode, 16 in 64-bit mode
    //     str - stack register,            pool: 8 (mostly legacy, used by fpu and mmx instructions)
    //     vec - vector register,           pool: 8 in 32-bit mode, 16 in 64-bit mode pre AVX512, 32 in 64-bit mode post AVX512
    //     omr - op-mask register,          pool: 8, only with AVX512

    // Sometimes you need to be a bit more explicit about which location registers get mapped to. Let's see some
    // examples...
    #asm {
        t: gpr === a; // 't' is pinned to gpr 'a' (0/al/ax/eax/rax)
        u: gpr === c; // 'u' is pinned to gpr 'c' (1/cl/cx/ecx/rcx)
        v: vec === 9; // 'v' is pinned to vec '9' (xmm9/ymm9/zmm9)

        // You can also write these inline. This is the verbose version of the inferred operands we saw earlier. You can
        // specify the entire declaration explicitly if you want.
        mov w: gpr === 15, 10;
    }

    // Certain instructions use implicit registers. In traditional assemblers, the onus is on the programmer to make
    // sure that those registers are available and have the right value before the instruction executes. Here, we
    // reframe it slightly. All registers used by an instruction are explicitly listed as operands, and any registers
    // that would have been implicitly referenced are required to be pinned.
    //
    // Let's see an example using the multiply instruction. This instruction looks like 'op1:op2 = op2 * op3', meaning
    // our 64-bit multiply gives us back a 128-bit result. In Traditional assemblers you'd only specify the third
    // operand explicitly. Here, we pass all three but the first two must be pinned to the correct locations.
    x: u64 = 197589578578;
    y: u64 = 895173299817;
    z: u64 = ---;
    #asm {
        x === a; // We pin the high level var 'x' to gpr 'a' as required by mul.
        z === d; // We pin the high level var 'z' to gpr 'd' as required by mul.
        mul z, x, y;
    }

    print_z := z;
    print_x := x;
    print("x * y = %:%\n", print_z, print_x);
}

memory :: () {
    // In x86 there are also memory operands in the form 'base + index * scale + displacement'. To indicate a memory
    // operand you wrap it in brackets, just like you would in a traditional assembler. The ordering of the expression
    // is considered rigid in that you cannot place the displacement first, or the base second, etc. This is to reduce
    // confusion since most of the fields are usually ambiguous identifiers.

    // Here are some rules on the different components:
    //     base  - required, must be a gpr
    //     index - optional, usually a gpr but can also be a vec in some instructions (more on that later)
    //     scale - only valid when there is an index, can be 1, 2, 4, or 8, when omitted it is default 1
    //     disp  - optional, a signed 8 or 32 bit integer byte offset

    dummy_array: [512] u8;
    dummy_array_pointer := dummy_array.data + 128;
    dummy_array_index := 10;

    DUMMY_SCALE :: size_of(u64);
    DUMMY_DISP  :: -10 * 2 + 13;

    // Let's enumerate some of the possible combinations...
    #asm {
        // We load 'b' and 'i' with some valid data since we don't want to crash the how-to when running through the
        // following instructions.
        mov b:, dummy_array_pointer;
        mov i:, dummy_array_index;

        mov t:, [b];
        mov t, [b + 10];
        mov t, [b - 10];
        mov t, [b + i];
        mov t, [b + i*1];
        mov t, [b + i*2];
        mov t, [b + i*4 + 10];
        mov t, [b + i*8 - 10];

        // We can also refer to high level vars directly in all of these slots. The scale and displacement must be
        // constant.
        mov t, [dummy_array_pointer + dummy_array_index*DUMMY_SCALE + DUMMY_DISP];

        // Taking that a step further, we can put these expressions inline by wrapping them in parens. The parens help
        // to distinguish between the components of the memory operand itself and whatever is being done in the constant
        // expressions.
        mov t, [dummy_array_pointer + dummy_array_index*(size_of(u64)) + (-10 * 2 + 13)];
    }

    // There are a few limitations with the current system. The first is stack addressing. You cannot directly access
    // variables that only exist on the stack via a memory operand. You also cannot access variables in global scope
    // via a rip-rel memory operand. These limitations are due to the current architecture and integration issues with
    // LLVM. This is not much of a functional problem in practice, it just means you have to manually load a raw pointer
    // variable with whatever you are trying to access before hand.
}

feature_flags :: () {
    // x86 doesn't have a single set of instructions. Instead, there are feature flags which say whether or not some
    // instruction or some set of instructions are present. #asm blocks only let you use instructions that you have
    // said are in your target feature set. This can be specified on a global build level, or on a per-block level.

    // To specifiy flags on a global build level, you setup the flags you want and pass them with the build options
    // during your build step. The flags are binary compatible with the feature bits specified by CPUID. See
    // Machine_X64.jai for a list of these. Set feature bits on build_options.machine_options.x86_features.leaves via
    // the enable_feature function in Machine_X64.jai.

    // In addition to the global build flags, you can specifiy additional features to enable for a specific #asm block.
    // The names match with the x86_Feature_Flag enum in Machine_X64.jai.

    // You do this by listing the feature flags like so...
    #asm AVX, AVX2 {
        // We can use AVX and AVX2 instructions here, even though our global build feature set didn't include them.
    }

    // To make use of this, you'll have to do a runtime check and branch on the feature flags. So in our example, we'd
    // have to check for AVX and AVX2 support on the current hardware, and branch to an older feature set if it isn't
    // available.

    // There is a helper function in Machine_X64.jai that can query the full set of relevant feature flags for you. It
    // is generally a good idea to do this once and cache the flags for testing later.
    cpu_info := get_cpu_info();

    if check_feature(cpu_info.feature_leaves, x86_Feature_Flag.AVX2) {
        #asm AVX2 {
            // Here the pxor gets the 256-bit .y version, since that is the default operand size with AVX. In an AVX512
            // block, the default operand size would be the 512-bit .z.
            pxor v1:, v1, v1;
        }
    }
    else {
        // AVX2 is not available on this processor, we have to run our fallback path...
    }
}

macros :: () {
    // While you can't pass registers to other procedures, you can #expand a macro that takes registers as arguments.
    // No additional mov's or other instructions will be generated as a result of this. The names are simply bound to
    // one another by the compiler and the same underlying register allocation is used.

    // The compiler currently defines __reg, but in the future will define reg, so we do that ourselves for now.
    reg :: __reg;

    // Here is our macro, it takes two regs. Any user can pass any two regs here and the add will get expanded at the
    // usage site with the correct mappings.
    add_the_two_regs :: (left: reg, right: reg) #expand {
        #asm {
            add left, right;
        }
    }

    // Setup some registers.
    #asm {
        mov a:, 12;
        mov b:, 18;
    }

    // Pass to the macro in any order.
    add_the_two_regs(b, a);

    // Print out the result.
    c: u64 = ---;
    #asm {
        mov c, b;
    }

    print_c := c;
    print("c: %\n", print_c);
}

compile_time_execution :: () {
    // #asm blocks can also be ran at compile time, just like any other code. The code is compiled to machine code once
    // and ran at native speed, so you can expect it to be just as fast.

    do_some_work :: (a: int, b: int) -> int {
        #asm {
            add a, b;
        }
        return a;
    }

    A :: #run do_some_work(10, 13);
    print("A: %\n", A);
}

advanced_features :: () {
    cpu_info := #ifx true get_cpu_info() else .{};

    // We are going to bail out early if your processor doesn't support AVX2 to avoid crashing the how-to.
    if !check_feature(cpu_info.feature_leaves, x86_Feature_Flag.AVX2) {
        print("Looks like you don't have AVX2 on this machine...\n");
        return;
    }

    // There are a handful of instructions that support a memory operand with a vector index. This is known as a VSIB
    // in the documentation. The only instructions that actually use it are scatters and gathers. Let's see an example
    // of this...

    // For demonstration, we'll gather from the original_float_array using the gather_indices. In practice you wouldn't
    // do something like this (it is just a shuffle), but we'd need some large set of actual data to demonstrate the
    // instruction otherwise.
    original_float_array := float.[ 1, 2, 3, 4, 5, 6, 7, 8 ];
    gather_indices := u32.[ 7, 6, 5, 4, 3, 2, 1, 0 ];
    gathered_float_array: [8] float;

    original_float_array_pointer := original_float_array.data;
    gather_indices_pointer := gather_indices.data;
    gathered_float_array_pointer := gathered_float_array.data;

    #asm AVX, AVX2 {
        // Load the register we are going to use for indexing from the gather_indices array, to keep things simple.
        movdqu vindex:, [gather_indices_pointer];

        // Get all 1's in our gather mask, we want to just let all lanes through.
        pcmpeqd gather_mask:, gather_mask, gather_mask;

        // Note that we scale our indices by 4. That is because these are byte offsets and we need to step in multiples
        // of the element size (32-bits), like any other memory operand. Also note that we don't use a displacement, but
        // it is supported and behaves just like a normal memory operand.
        gatherdps gather_dest:, [original_float_array_pointer + vindex*4], gather_mask;
        movdqu [gathered_float_array_pointer], gather_dest;
    }

    print("gather result: %\n", gathered_float_array);

    // AVX512 introduced a new encoding called EVEX that supports a number of new features. We support these features
    // and all instructions in the feature set. Let's go over some of them here...

    // We are going to bail out early if your processor doesn't support AVX512 to avoid crashing the how-to.
    if !check_feature(cpu_info.feature_leaves, x86_Feature_Flag.AVX512F) {
        print("Looks like you don't have AVX512F on this machine...\n");
        return;
    }

    // We use this array for printing out vector register contents.
    array: [16] s32;
    array_pointer := array.data;

    // We use this value for demonstrating memory broadcasting and rounding modes. Taking the address of the variable
    // gives us a pointer to a location on the stack we can use for reading.
    value: float = 127.53;
    value_pointer := *value;

    // One of the EVEX features is the 'broadcasting' flag. When this flag is set (and the instruction supports it), the
    // memory operand will only load a single value and duplicate it to each lane in the vector register. An instruction
    // form that accepts a broadcasting memory operand will look something like 'mem!' in the compiler print out for
    // supported forms. The broadcast is optional in these cases, when not supplied the memory operand behaves normally.
    #asm AVX512F {
        // The broadcast '!' gets rid of the extra broadcastss instruction we otherwise need (see the following examples).
        cvtps2dq v1:, [value_pointer]!;
        movdqu [array_pointer], v1;
    }

    print("% broadcasted and converted to %\n", value, array);

    // Another EVEX feature has to do with floating point operations. Some instructions take an operand that supports
    // either SAE (suppress all exceptions) or an ER (explicit rounding mode). Here are the different combinations:
    //
    //     !  - SAE
    //     !n - SAE + round to nearest (even)
    //     !d - SAE + round down (toward -inf)
    //     !u - SAE + round up (toward +inf)
    //     !z - SAE + round towards zero (truncate)
    //
    // You can only use the stand-alone SAE flag when the instruction doesn't support the full explicit rounding modes.
    // The form print-out from the compiler shows this as 'vec!'. When the form print-out shows 'vec!r', the rounding
    // mode is required (replace r with whatever rounding mode you want).

    // Here is an example using an explicit rounding mode...
    #asm AVX512F {
        broadcastss v2:, [value_pointer];
        cvtps2dq v3:, v2 !z;
        movdqu [array_pointer], v3;
    }

    print("% broadcasted and truncated to %\n", value, array);

    // A more well known feature in the EVEX encoding is masking. Mask registers can be applied to the first operand of
    // an instruction that supports it. When an operand can be masked, the form print-out will show something like
    // 'vec&' or 'vec&*'. The & version is the merging operator while the &* version is the zeroing operator. Not all
    // instructions support both, and some instructions actually require the zeroing flag to be set.

    // Let's run that same example but this time we'll mask off some of the elements...
    #asm AVX512F {
        // We init our bitmask by just loading an immediate and kicking it over to a mask register.
        mov mask_init:, 0b0101_0101_0101_0101;

        // We want the 16-bit version of kmov here since we are masking 32-bit elements (a 512-bit vector has 16 elements).
        // There are also b/d/q versions of the k-instructions for when you are doing operations on different element
        // sizes. For example, a byte sized mask can be used with 64-bit elements (requires AVX512DQ), a qword sized
        // mask can be used with 8-bit elements (requires AVX512BW), etc.
        kmovw mask:, mask_init;

        broadcastss v4:, [value_pointer];
        cvtps2dq v5: &* mask, v4 !z;
        movdqu [array_pointer], v5;
    }

    print("% broadcasted and truncated with masking to %\n", value, array);
}

polymorphic_sizes :: () {
    // Way up above we discussed tagging instruction mnemonics with explicit sizes
    // like 8 or 32 or 128.

    // Sizes can also be determined by variables from the higher-level language,
    // which can help make polymorphic functions terse (among other advantages).

    // Here's a function that counts bits in an integer (it's copied from modules/Bit_Operations.jai,
    // with some modifications):

    my_popcount :: (value: $T) -> s32 #expand {
        Bits :: size_of(T)*8;

        result: s32;

        #if Bits == 8 #asm {
            // There is no popcnt.8, so we need to move into 16 bits.
            // We will explicitly specify the 16 as above.
            movzxbw      two_bytes:, value;
            popcnt.16    result, two_bytes;
        } else #asm {
            // We can say ?Bits at the end of the mnemonic
            // to specify that the size is Bits.
            popcnt?Bits  result, value;

            // Or, we can just use T; if we put a Type after ?
            // we'll get the size of T in bits.

            popcnt?T     result, value;

            // (Yes, we just redundantly computed 'result', but
            // we wanted to show both ways of sizing the operation!)
        }

        return result;
    }

    // Because my_popcount is polymorphic, it will work on values
    // of size 8, 16, 32, and 64. (If you pass something bigger,
    // you'll get an error that there's no variant of popcnt at that size.)

    a: u32 = 0xcafe;
    b: s64 = 0xcafebabe00c0ffee;

    a_result := my_popcount(a);
    b_result := my_popcount(b);

    print("a_result: %\n", a_result);
    print("b_result: %\n", b_result);
}

main :: () {
    basics();
    immediates();
    allocation_and_pinning();
    memory();
    feature_flags();
    macros();
    compile_time_execution();
    advanced_features();
    polymorphic_sizes();
}

#import "Basic";
#import "Machine_X64";
		</code></pre>
	<pre id="notIt"><code class="language-jai">#import "Basic";
/*
	Jai Bucket Allocator
	====================

	Inspired by something Jon mentioned on a compiler stream about an allocator that can handle any type, but is not as complex as a full malloc implementation.
	This allocator has buckets of fixed-sized (power of 2) chunks of memory, and allocates from the smallest one to fit the requested size.
	The sizes available and the number of each is controled with the SIZES_CONFIG param - see below.  Everything is dynamically created off this config so there&apos;s no waste.

	It does not support create_/destroy_heap() nor is it yet tested for multithreading.

	It supports allocating, freeing, and also resizing:
		- If the new size still fits in the same sized bucket, nothing moves.
		- If the new size needs a larger bucket, a new allocation is made and the data copied into it.
		- If the new size fits in a smaller bucket, either nothing happens or a new allocation is made and the smaller size worth of data is copied, depending on HONOUR_REALLOC_SHRINK - see below.

	It also optionally records statistics that can then be printed, aiding in tuning the SIZES_CONFIG to your code&apos;s runtime needs, if you want to configure it as lean as needed for example.

	Allocations are managed via a free-list and used-list, which means freeing has an O(n) component at the moment.  Might consider bit arrays if this becomes a performance issue.

	Note that allignment is only ensured if the initial allocation of the BucketArray instance is also aligned.

	Module Parameters:
	------------------
		Currently none, tho I&apos;m considering moving SIZES_CONFIG to be one instead of a Program Parameter.
		This would allow there to be multiple instances with different configs, rather than the current paradigm that expects there to only ever be one [configuration].

	Program Parameters:
	-------------------
		SIZES_CONFIG:
			This int array defines how many sizes of bucket there will be (by its length), as well as how many of each size there will be (by the values).
			The smallest bucket is for 16byte allocations, so the first entry in the array controls how many of those will exist.  The next entry, if it is present, is for 32byte allocations, etc.
			If an entry is 0 then that size will be excluded and allocations that would want that size will fail.
			As many buckets as non-zero entries will be created.
			Default is .[65536, 32768, 16384, 8192, 4096, 2048, 1024, 512, 256] which will allocate 1MiB for each size from 16bytes to 4096bytes.

		USE_UNMAPPING_ALLOCATOR:
			Like other allocators, stomps freed memory with 0xCC if true, to aid in memory debugging.
			Defaults to false.

		HONOUR_REALLOC_SHRINK:
			If false, a realloc() that asks for less memory won&apos;t actually do anything, leaving the allocation where it is.
			If true, a shrink will cause a reallocation (if it would cause a difference in bucket size).
			Note that realloc()s that don&apos;t alter the bucket size the allocation is in, either grows or shrinks, will always be NOPs anyway, regardless of this parameter.

		RECORD_STATS:
			If true, we allocate a bit more RAM to hold usage stats and update them as things are allocated/reallocated/freed.
			It also defines printStats(*BucketAllocator, compact=false) that can then print the stats.
			This is useful if you want to initialise to a configuration that allocates no more than it needs to for example.

		ALLOW_OVERFLOW:
			If true, an attempt to allocate what would go into a certain sized bucket is allowed to overflow to the next size up if there are no entries left.
			This continues until a successful allocation or we run out of sizes to try.
			Defaults to true.

	Example:
	--------
	The usual pattern for use would be something like:
		#import "Bucket_Allocator"()(SIZES_CONFIG = int.[10,10,10,10]);

		...

		newContext := context;
		newContext.allocator = makeBucketAllocator();
		push_context newContext {
			...do stuff using the new allocator, remembering that everything including print will then use it...
		}

	Or alternatively, using the push_allocator macro:
		...
		push_allocator(makeBucketAllocator());
		...do stuff using the new allocator, till the end of the current scope...

	API:
	----
		makeBucketAllocator() -&gt; Allocator
			Allocates a BucketAllocator (via New()) and returns an Allocator struct configured to use it.

		freeBucketAllocator(Allocator)
			Frees the BucketAllocator configured in the supplied Allocator struct.
			Not expected to be needed much but is here for completeness.

		init(*BucketAllocator)
			For in case you want to create the BucketAllocator yourself but still need to initialise it.
			Note that this sets up the initial free-lists so is necessary to call before you can use the BucketAllocator instance; makeBucketAllocator() calls this for you.

		deInit(*BucketAllocator)
			Frees the supplied allocator by calling free() on it.
			Again, not expected to be needed much but is here for completeness.

		#if RECORD_STATS {
			printStats(*BucketAllocator, compact=false)
				Prints either detailed or compact stats about the supplied BucketAllocator.
				Compact stats just print the used/total counts and a percentage full per bucket size.
		}

		bucket_allocator_proc(Allocator_Mode, s64, s64, *void, *void) -&gt; *void
			The Allocator_Proc that drives the BucketAllocator.
			Not expected to be called directly, but rather via calls to alloc(s64[, *Allocator]), realloc(*void, s64, s64[, *Allocator]), free(*void[, *Allocator]), and get_capabilities(*Allocator).

//LATER: logging?, //MAYBE: extra storage when full (iff RECORD_STATS?)?
//QUESTION: allow underflow?  Probably means adding a setting for which to prefer.

*/

// Defaults are 1MiB each size.  Not sure if that makes sense yet.
//QUESTION: should SIZES_CONFIG be a module or a program param?
#module_parameters ()(
	SIZES_CONFIG : [$N]int = .[65536, 32768, 16384, 8192, 4096, 2048, 1024, 512, 256],
	USE_UNMAPPING_ALLOCATOR := false,
	HONOUR_REALLOC_SHRINK := false.(bool),
	RECORD_STATS := false,
	ALLOW_OVERFLOW := true
);

bucket_allocator_proc :: (mode: Allocator_Mode, requested_size: s64, old_size: s64, old_memory: *void, allocator_data: *void) -&gt; *void {
	bucketAllocator := cast(*BucketAllocator)allocator_data;

	if #complete mode == {
		case .ALLOCATE; #through;
		case .RESIZE;
			return allocate(bucketAllocator, mode, requested_size, old_size, old_memory);
		case .FREE;
			free(bucketAllocator, old_memory);
			#if USE_UNMAPPING_ALLOCATOR {
				assert((cast(*u8)old_memory).* == 0xCC, "Didn&apos;t wipe on free correctly!");
			}
			return null;

		case .STARTUP;	#through;
		case .SHUTDOWN;
			return null;

		case .THREAD_START;	#through;
		case .THREAD_STOP;
			assert(false, "Threading not supported [yet?].");

		case .CREATE_HEAP;	#through;
		case .DESTROY_HEAP;
			assert(false, "Heaps not supported.");

		case .IS_THIS_YOURS;
			return cast(*void) cast(s64) (findBucket(bucketAllocator, old_memory) &gt; 0);

		case .CAPS;
			if old_memory {
				(cast(*string)old_memory).* = CAPS_VERSION_STRING;
			}
			return cast(*void)(Allocator_Caps.FREE|.ACTUALLY_RESIZE|.IS_THIS_YOURS);
	}

	assert(false, "unreachable");
	return null;
}

#if RECORD_STATS {
	printStats :: (using bucketAllocator: *BucketAllocator, compact := false) {
		print("BucketAllocator @%\n", bucketAllocator);
		if (compact) {
			for 0..SIZES_CONFIG.count - 1 {
				#if ALLOW_OVERFLOW {
					print(
						"  %: %/% = %4%% (%)\n",
						formatInt(it + 4, minimum_digits=2),
						stats[it].highWaterMark,
						SIZES_CONFIG[it],
						(10000 * stats[it].highWaterMark / SIZES_CONFIG[it]) / 100.0,
						stats[it].overflows
					);
				} else {
					print(
						"  %: %/% = %4%%\n",
						formatInt(it + 4, minimum_digits=2),
						stats[it].highWaterMark,
						SIZES_CONFIG[it],
						(10000 * stats[it].highWaterMark / SIZES_CONFIG[it]) / 100.0
					);
				}
			}
		} else {
			baType: *Type_Info_Struct = xx type_info(BucketAllocator);
			for 0..SIZES_CONFIG.count - 1 {
				field := get_field(baType, tprint("size%Bucket", cleanFormat(it + 4)));
				print("  Size: % (%..%)\n", it + 4, bucketAllocator + field.offset_in_bytes, bucketAllocator + field.offset_in_bytes + field.type.runtime_size - 1);
				print("    Allocated : %\n", stats[it].allocated);
				print("    Freed     : %\n", stats[it].freed);
				print("    High Water: %\n", stats[it].highWaterMark);
				print("    LIMIT     : %\n", SIZES_CONFIG[it]);
				print("      %%       : %\n", (10000 * stats[it].highWaterMark / SIZES_CONFIG[it]) / 100.0);
				print("    Reallocate:\n");
				print("      Requested: %\n", stats[it].reallocateRequests);
				print("      NOPs     : %\n", stats[it].reallocateRequests - (stats[it].reallocateGrowsNeeded + stats[it].reallocateShrinksNeeded));
				print("      Grows    : %\n", stats[it].reallocateGrowsNeeded);
				#if HONOUR_REALLOC_SHRINK {
					print("      Shrinks  : %\n", stats[it].reallocateShrinksNeeded);
				}
				#if ALLOW_OVERFLOW {
					print("    Overflows: %\n", stats[it].overflows);
				}
			}
		}
	}
}

makeBucketAllocator :: () -&gt; Allocator {
	ba := New(BucketAllocator);
	init(ba);
	return .{ proc = bucket_allocator_proc, data = xx ba };
}

freeBucketAllocator :: (a: Allocator) {
	assert(a.proc == bucket_allocator_proc &amp;&amp; a.data, "Not a BucketAllocator!");
	ba: cast(*BucketAllocator) a.data;
	deInit(ba);
}

init :: (using bucketAllocator: *BucketAllocator) {
	assert(allocator.proc == null, "Already initialised!");
	assert(context.allocator.proc != bucket_allocator_proc, "Can&apos;t allocate off ourselves!");
	allocator = context.allocator;

	#insert #run generateSizeCalls("\tinitBucket(*size%Bucket);\n");
}

deInit :: (using bucketAllocator: *BucketAllocator) {
	free(bucketAllocator, allocator);
}

#scope_file

BucketSize :: #type,distinct u8;
OldBucketSize :: #type,isa BucketSize;

MAX_SIZE : BucketSize : SIZES_CONFIG.count + 3;

CAPS_VERSION_STRING :: "Bucket_Allocator v1.1";

initBucket :: (bucket: *BucketSet($S, $L)) {
	for &lt; L - 1..0 {
		ref := *bucket.refs[it];
		ref.item = xx *bucket.items[it];
		ref.next = bucket.freeList;
		bucket.freeList = ref;
	}
	bucket.usedList = null;
}

#if HONOUR_REALLOC_SHRINK {
	reallocShouldNotMove :: (oldSize: OldBucketSize, size: BucketSize) -&gt; bool {
		return oldSize == size;
	}
} else {
	reallocShouldNotMove :: (oldSize: OldBucketSize, size: BucketSize) -&gt; bool {
		return oldSize &gt;= size;
	}
}

allocate :: (using bucketAllocator: *BucketAllocator, mode: Allocator_Mode, requestedSize: s64, oldSize: s64, oldMemory: *void) -&gt; *void {
	size := calculateSize(requestedSize);
	ref: *BucketItemRef;

	if mode == .RESIZE {
		oldSize, oldRef := findBucket(bucketAllocator, oldMemory, requestedSize);
		assert(oldSize &gt; 0, "Not our memory!");
		#if RECORD_STATS {
			index := size - 4;
			stats[index].reallocateRequests += 1;
		}

		if reallocShouldNotMove(oldSize, size) {
			return oldMemory;
		}


		#if RECORD_STATS {
			if oldSize &lt; size {
				stats[index].reallocateGrowsNeeded += 1;
			} else {
				stats[index].reallocateShrinksNeeded += 1;
			}
		}
		ref = doAllocateNew(bucketAllocator, size);

		memcpy(ref.item, oldRef.item, 1&lt;&lt;min(size, oldSize));	// @Speed: if the requested size lots smaller than bucket&apos;s size we are copying more than we need.  Not sure we care.
	} else {
		ref = doAllocateNew(bucketAllocator, size);
	}

	return ifx ref  ref.item else null;
}

#if ALLOW_OVERFLOW {
	doAllocateNew :: inline (bucketAllocator: *BucketAllocator, size: BucketSize) -&gt; *BucketItemRef {
		ref := allocateNew(bucketAllocator, size);
		if ref  return ref;


		origSize := size;
		while !ref &amp;&amp; size &lt; MAX_SIZE {
			size += 1;
			ref = allocateNew(bucketAllocator, size);
		}

		if ref {
			log("BucketAllocator: Overflowed allocation of BucketSize % into size %!", origSize, size, flags = .WARNING);
			#if RECORD_STATS {
				for origSize - 4..size - 5 {
					bucketAllocator.stats[it].overflows += 1;
				}
			}
		}
		return ref;
	}
} else {
	doAllocateNew :: inline (bucketAllocator: *BucketAllocator, size: BucketSize) -&gt; *BucketItemRef {
		return allocateNew(bucketAllocator, size);
	}
}

calculateSize :: (requestedSize: s64) -&gt; BucketSize {
	assert(requestedSize &gt; 0, ifx requestedSize then "Can&apos;t allocate negative size!" else "Can&apos;t allocate 0 size!");
	//TODO: #asm to find the top bit...
	size : BucketSize = 4;
	length := 1&lt;&lt;size;
	while length &lt; requestedSize {
		size += 1;
		length &lt;&lt;= 1;
	}

	return size;
}

allocateNew :: inline (using bucketAllocator: *BucketAllocator, size: BucketSize) -&gt; *BucketItemRef {
	#insert #run generateSizeCalls(
		preamble="\tif size == {\n",
		format="\t\tcase %1;\n\t\t\treturn allocateNew(bucketAllocator, *size%1Bucket);\n",
		postamble="\n\t\tcase;\n\t\t\tassert(false, \"Bad size %%!\", size);\n\t\t\treturn null;\n\t}\n"
	);
}

allocateNew :: (using bucketAllocator: *BucketAllocator, bucket: *BucketSet($S, $L)) -&gt; *BucketItemRef {
	#if RECORD_STATS {
		index :: S - 4;
		stats[index].allocated += 1;
		mark := stats[index].allocated - stats[index].freed;
		if mark &gt; stats[index].highWaterMark {
			stats[index].highWaterMark = mark;
		}
	}

	ref := bucket.freeList;
	if !ref {
		return null;
	}


	bucket.freeList = ref.next;
	ref.next = bucket.usedList;
	bucket.usedList = ref;

	return ref;
}

free :: (using bucketAllocator: *BucketAllocator, oldMemory: *void) {
	oldSize := findBucket(bucketAllocator, oldMemory, -1);
	assert(oldSize &gt; 0, "Not our memory!");
	#if RECORD_STATS {
		stats[oldSize - 4].freed += 1;
	}
}

findBucket :: (using bucketAllocator: *BucketAllocator, memory: *void, newSize: int = 0) -&gt; OldBucketSize, *BucketItemRef {
	testBucket :: (memory: *void, bucket: *BucketSet($S, $L)) #expand {
		itemSize :: 1 &lt;&lt; S;
		offset : s64 = cast(s64) memory - cast(s64) *bucket.items[0];
		inside := offset &gt;= 0 &amp;&amp; offset &lt; itemSize * SIZES_CONFIG[S - 4];
		if (inside) {
			ref: *BucketItemRef;
			assert(offset % itemSize == 0, "Don&apos;t free the middle of an item!");
			if (`newSize &lt; 0 || !reallocShouldNotMove(S, calculateSize(`newSize))) {
				ref = unallocateRef(bucket, memory);
			}

			`return S, ref;
		}
	}

	#insert #run generateSizeCalls("\ttestBucket(memory, *size%Bucket);\n");

	return 0, null;
}

unallocateRef :: (using bucket: *BucketSet($S, $L), memory: *void) -&gt; *BucketItemRef {
	//NOTE: @Speed: finding the allocated bucket is O(n)...
	prev: *BucketItemRef;
	ref := usedList;

	assert(ref != null, "Nothing allocated here!");
	while (ref &amp;&amp; memory != xx ref.item) {
		prev = ref;
		ref = ref.next;
	}
	assert(ref != null, "Didn&apos;t find the ref!");

	if prev {
		prev.next = ref.next;
	} else {
		usedList = ref.next;
	}

	ref.next = freeList;
	freeList = ref;

	#if USE_UNMAPPING_ALLOCATOR {
		memset(ref.item, 0xCC, 1&lt;&lt;S);
	}

	return ref;
}

cleanFormat :: inline (value: int) -&gt; FormatInt {
	// Just in case the caller has modified the default print_style in a way that might break the generated code, reset it:
	return .{value=value};
}

generateSizeCalls :: (format: string, preamble := "", postamble := "") -&gt; string {
	builder: String_Builder;
	size : BucketSize = 4;

	print_to_builder(*builder, preamble);

	for SIZES_CONFIG {
		if it {
			print_to_builder(*builder, format, cleanFormat(xx size));
		}
		size += 1;
	}

	print_to_builder(*builder, postamble);

	return builder_to_string(*builder);
}

generateSizeFields :: (format: string) -&gt; string {
	builder: String_Builder;
	size : BucketSize = 4;

	for SIZES_CONFIG {
		if it {
			print_to_builder(*builder, format, cleanFormat(xx size), cleanFormat(it));
		}
		size += 1;
	}

	return builder_to_string(*builder);
}

#scope_export

BucketAllocator :: struct {
	#as allocator: Allocator;

	#insert #run generateSizeFields("\tsize%1Bucket: BucketSet(size = %1, length = %2);\n");

	#if RECORD_STATS {
		stats: [SIZES_CONFIG.count] BucketStats;
	}
}

BucketSet :: struct(size: int, length: int) {
	items: [length] [1 &lt;&lt; size] u8;
	refs: [length] BucketItemRef;
	freeList: *BucketItemRef;
	usedList: *BucketItemRef;
}

BucketItemRef :: struct {
	item: *u8;
	next: *BucketItemRef;
}

#if RECORD_STATS {
	BucketStats :: struct {
		allocated: int;
		reallocateRequests: int;
		reallocateGrowsNeeded: int;
		#if HONOUR_REALLOC_SHRINK {
			reallocateShrinksNeeded: int;
		}
		freed: int;
		highWaterMark: int;
		#if ALLOW_OVERFLOW {
			overflows: int;
		}
	}
}
		</code></pre>
	<script type="text/javascript" src="debug.js"></script>
	<!--<script type="text/javascript">
		hljs.highlightAll();
	</script>-->
</body>

</html>
